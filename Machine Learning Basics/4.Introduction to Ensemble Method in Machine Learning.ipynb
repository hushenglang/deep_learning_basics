{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Ensemble Method in Machine Learning\n",
    "> In this notebook, it will show you a awesome learning algorithm \"Ensemble Method\". **This Algorithm is very important!**\n",
    "\n",
    "> 1.What is Ensemble Method?\n",
    "\n",
    "> 2.Brief of Voting and Averaging Based Ensemble Method\n",
    "\n",
    "> 2.1Implementaion of Voting Based Ensemble Methods with SK-learn\n",
    "\n",
    "> 3.Brief of Bagging Ensemble Method\n",
    "\n",
    "> 3.1Brief of Random Forest Algorithm\n",
    "\n",
    "> 3.2Implementation of Random Forest with SK-learn\n",
    "\n",
    "> 4.Brief of Boosting Method\n",
    "\n",
    "> 4.1Implementation of Adaboost with SK-learn\n",
    "\n",
    "> 4.2Implementation of Gradient Tree Boosting\n",
    "\n",
    "> 5.Break it down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.What is Ensemble Method?\n",
    "* Ensemble methods are learning algorithms that construct **a set of classiers** and then classify new data points by taking a **weighted vote** of their predictions.\n",
    "\n",
    "* In practice, we will create **multiple models** and then **combine them** to produce improved results, and it usually produces more accurate solutions than a single model. \n",
    "\n",
    "* Some widely known methods of ensemble: **Voting&Averaging, Bagging(Random Forest), Boosting(GradientBoosting)** and **Stacking(it will not be briefed in this notebook)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Brief of Voting and Averaging Based Ensemble Methods.\n",
    "* **Voting and Averaging** are two of the **easiest ensemble methods**. \n",
    "* **Voting** is used for **classification** and **Averaging** is used for **regression**.\n",
    "\n",
    "\n",
    "**Steps of create this model:**\n",
    "1. First step is to create **multiple classification/regression models** using some training dataset. Each base model can be created using the **same dataset with different algorithms**, or any other method. \n",
    "\n",
    "2. Second step is **Voting/Averaging to get final prediction**, there are few different ways for Voting/Averaging**(Majority Voting, Weighted Voting, Simple Averaging, Weighted Averaging)**.\n",
    "\n",
    "<img src=\"asset/Votting.png\",width=500,height=500, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementaion of Voting Ensemble Methods with SK-learn\n",
    "SK-Learn API: http://scikit-learn.org/stable/modules/ensemble.html#votingclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# voting='hard' means majority voting; voting=\"soft\" is recommended\n",
    "eclf = VotingClassifier(estimators=[('logistic', clf1), ('randomForest', clf2), ('gaussianNB', clf3)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.04) [Logistic Regression]  K-Fold: [ 0.87179487  0.87179487  0.88888889  0.97222222]\n",
      "Accuracy: 0.93 (+/- 0.03) [Random Forest]  K-Fold: [ 0.92307692  0.97435897  0.88888889  0.94444444]\n",
      "Accuracy: 0.91 (+/- 0.02) [naive Bayes]  K-Fold: [ 0.8974359   0.8974359   0.91666667  0.94444444]\n",
      "Accuracy: 0.95 (+/- 0.02) [Ensemble]  K-Fold: [ 0.94871795  0.94871795  0.91666667  0.97222222]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=4, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]  K-Fold: %s\" % (scores.mean(), scores.std(), label, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Brief of Bagging Ensemble Methods\n",
    "* The name **Bagging**, also known as **“Bootstrap Aggregating”**.\n",
    "\n",
    "* Bagging is a technique used to **reduce the variance of our predictions** by combining the result of multiple classifiers modeled on different sub-samples of the same data set.\n",
    "\n",
    "* In the bagging algorithm, **the first step** is create multiple dataSets; **the second step** is build classifiers; **the last step** is combine classifiers which is collecting all classifiers' prediction and do mean operation for final prediction.\n",
    "\n",
    "* **Bagging methods** work best with **strong and complex models** (e.g., fully developed decision trees).\n",
    "\n",
    "* **Random Forest algorithm** uses the bagging technique with some differences.\n",
    "\n",
    "<img src=\"asset/Bagging.png\",width=500,height=500, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1Brief of Random Forest Algorithm\n",
    "**Pros**:\n",
    "\n",
    "* This algorithm can solve both type of problems i.e. **classification and regression** and does a decent estimation at both fronts.\n",
    "\n",
    "* One of benefits of Random forest which excites me most is, the power of **handle large data set with higher dimensionality**. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
    "\n",
    "* It has an effective method for **estimating missing data** and maintains accuracy when a large proportion of the data are missing.\n",
    "\n",
    "* It has methods for **balancing errors** in data sets where classes are imbalanced.\n",
    "\n",
    "* The capabilities of the above can be extended to **unlabeled data**, leading to **unsupervised clustering, data views and outlier detection.**\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "* It surely does a **good job at classification** but **not as good as for regression problem** as it does not give precise continuous nature predictions. In case of regression, it doesn’t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.\n",
    "\n",
    "* Random Forest can feel like a **black box approach** for statistical modelers – you have very little control on what the model does. **You can at best – try different parameters and random seeds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2Implementation of Random Forest with SK-learn\n",
    "SK-Learn API: http://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees\n",
    "\n",
    "SK-Learn Sample Generator API: http://scikit-learn.org/stable/datasets/index.html#sample-generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Score: 0.9828000000000001\n"
     ]
    }
   ],
   "source": [
    "# build Decision Tree Model for comparing with the Random Forest later on\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(\"Decision Tree Model Score: {0}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Score: 0.9998000000000001\n"
     ]
    }
   ],
   "source": [
    "# build RandomForest Model\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(\"Random Forest Model Score: {0}\".format(scores.mean()))                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Model Score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# build Extra Tree Model\n",
    "# note: This algorithm compared with Random Forest usually reduce the variance of the model a bit more, \n",
    "# at the expense of a slightly greater increase in bias.\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(\"Extra Tree Model Score: {0}\".format(scores.mean()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the code above, we can see that Random Forest model is better than single Desicion Tree Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Brief of Boosting Method\n",
    "\n",
    "* The term “boosting” is used to describe a family of algorithms which are able to **convert weak models to strong models**.\n",
    "\n",
    "* Boosting incrementally builds an ensemble by **training each model with the same dataset** but where **the weights of instances are adjusted according to the error of the last prediction**. The main idea is forcing the models to focus on the instances which are hard. **Unlike bagging, boosting is a sequential method**, and so you can not use parallel operations here.\n",
    "\n",
    "* **Adaboost** is a widely known boosting method algorithm.\n",
    "\n",
    "* Mostly, **decision tree algorithm** is preferred as a **base algorithm for Adaboost** and in **sklearn library the default base algorithm for Adaboost is decision tree**\n",
    "\n",
    "<img src=\"asset/AdaBoost.png\",width=500,height=500, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1Implementation of Adboost with SK-learn\n",
    "Adaboost SK-Learn API:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95996732026143794"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2Implementation of Gradient Tree Boosting\n",
    "SK-Learn API:http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting\n",
    "\n",
    "The advantages of GBRT are:\n",
    "* Natural handling of data of mixed type (= heterogeneous features)\n",
    "* Predictive power\n",
    "* Robustness to outliers in output space (via robust loss functions)\n",
    "\n",
    "The disadvantages of GBRT are:\n",
    "* Scalability, due to the sequential nature of boosting it can hardly be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673202614379085"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Break it down:\n",
    "\n",
    "* In practice, **ensemble method** usually produces **more accurate solutions than a single model** would.\n",
    "\n",
    "* **Voting** is the **easiest method of ensemble**. it **creates different classifier based on the same dataset**, and finally combine the predictions and give the final prediction based on voting.\n",
    "\n",
    "* **Bagging methods** **work best with strong and complex models**. it **splits the dataset into sub-dataset**, and using multiple same algorithm trained on different sub-dataset to build a ensembeled model. \n",
    "\n",
    "* **Random Forest** is one of the most popular Bagging method, and it is also **black-box algorithm**, so in practice we'd better try **different hyper-params** to train a better model.\n",
    "\n",
    "* **Boosting method** convert **weak models to strong models**, and it trains each model with the same dataset. **Adaboost** is widely known boosting method algorithm\n",
    "\n",
    "* **Ensemble method** is often **not preferred in the industries** where **interpretability** is more important. however, when in practice if the case which **higher accuracy is more important than interpretability**, then consider to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: \n",
    "\n",
    "https://www.toptal.com/machine-learning/ensemble-methods-machine-learning\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/#five\n",
    "\n",
    "http://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
