{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductino to XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Boosting & Gradient Boosting?\n",
    "* Boosting is an ensemble technique where **new models are added to correct the errors made by existing models**. Models are added sequentially until no further improvements can be made\n",
    "\n",
    "* Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. \n",
    "\n",
    "* It is called gradient boosting because it ** uses a gradient descent algorithm** to minimize the loss when adding new models.\n",
    "\n",
    "* This approach supports both regression and classification predictive modeling problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use XGBoost?\n",
    "* **Execution Speed**: Generally, XGBoost is fast. Really fast when compared to other implementations of gradient boosting.\n",
    "* **Model Performance**: XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems.\n",
    "* **Scalibility**: Supports distributed training on multiple machines, including AWS, GCE, Azure, and Yarn clusters. Can be integrated with Flink, Spark and other cloud dataflow systems.\n",
    "* The XGBoost library implements the gradient boosting decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "# load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, y_train = X[:135], y[:135]\n",
    "X_test, y_test = X[135:], y[135:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "clf.fit(X_train, y_train)\n",
    "# make prediction\n",
    "preds = clf.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions\n",
    "    \n",
    "https://xgboost.readthedocs.io/en/latest/model.html\n",
    "    \n",
    "http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\n",
    "    \n",
    "http://xgboost.readthedocs.io/en/latest/build.html\n",
    "    \n",
    "http://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
