{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introductino to Decision Tree Algorithm\n",
    "> In this note, it will show you the basics of Decision Tree Algorithm, Random Forest Algorithm and Gradient Boosting Algorithm.\n",
    "\n",
    "> 1.What is a Decision Tree? How does it work?\n",
    "\n",
    "> 2.How to choose between tree based models and linear models?\n",
    "\n",
    "> 3.Pros and Cons of Desicion Tree?\n",
    "\n",
    "> 4.Desicion Tree implementation with SK-learn\n",
    "\n",
    "> 5.Break it down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.What is a Decision Tree? How does it work?\n",
    "**Tree based learning algorithms** are considered to be one of the best and mostly used supervised learning methods. Tree based methods empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).\n",
    "\n",
    "**Decision Tree** works for both **categorical** and **continuous** input and output variables. In this technique, we **split the population or sample into two or more homogeneous sets **(or sub-populations) based on **most significant splitter / differentiator **in input variables.\n",
    "\n",
    "<img src=\"asset/Descion_Tree1.png\",width=800,height=800, style=\"float: left;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy** controls how Decision Tree decide **where to split the data**, and **Entropy** itself is to measure the **impurity** in a bunch of examples. **The larger Entropy value is the more impure the examples are**, and Engtropy value is between **0~1**.\n",
    "\n",
    "<img src=\"asset/Desicion_Tree_Entropy.png\",width=400,height=400, style=\"float: left;\">\n",
    "\n",
    "\n",
    "<img src=\"asset/Desicion_Tree_Information_Gain.png\",width=500,height=500, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How to choose between tree based models and linear models?\n",
    "\n",
    "* If the relationship between dependent & independent variable is well approximated by a linear model, linear regression will outperform tree based model.\n",
    "\n",
    "* If there is a **high non-linearity & complex relationship ** between dependent & independent variables, a tree model will outperform a classical regression method.\n",
    "\n",
    "* If you need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Pros and Cons of Desicion Tree?\n",
    "**Pros**:\n",
    "* Simple to understand and to interpret. Trees can be visualised.\n",
    "\n",
    "* Requires **little data preparation**. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "\n",
    "* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "\n",
    "**Cons**:\n",
    "* Decision-tree learners can create **over-complex trees** that do not generalise the data well. This is called **overfitting**. \n",
    "\n",
    "* Decision trees can be **unstable** because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "\n",
    "* The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "* Decision tree learners create **biased trees if some classes dominate**. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Desicion Tree implementation with SK-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libs\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "iris = load_iris()\n",
    "X, y= iris.data, iris.target\n",
    "train_feature, test_feature, train_label, test_label= train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build and train model\n",
    "# you can change the criterion to see the difference bewteen 'entropy' and 'gini'\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy') \n",
    "clf = clf.fit(train_feature, train_label)\n",
    "clf.score(train_feature, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make prediction and test the accuracy\n",
    "pred = clf.predict(test_feature)\n",
    "accuracy_score(test_label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Break it down\n",
    "* Decision Tree split the population or sample into two or more homogeneous sets using **Entropy**\n",
    "* Decision Tree requires **little data preparation**\n",
    "* Decision Tree is better to solve **non-linear problem**\n",
    "* Decision Tree tend to be **over-fitting** and **unstable**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
