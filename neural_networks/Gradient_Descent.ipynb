{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "This notebook will show you the underlying implementation of Gradient Descent for Linear function and Sigmoid function.\n",
    "\n",
    "If you have any suggestions, welcome to contact [hushenglang@gmail.com](), or pull request on github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear equation\n",
    "> y = wx + b \n",
    "\n",
    "> w is slope, b is y-intercept\n",
    "\n",
    "### error function - mean squared error\n",
    "should be aware that there are lots of error function for different tasks, **mean squared error** function is the one\n",
    "\n",
    "we did not choose **sum squared error**, because if using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge.\n",
    "\n",
    "for linear regression\n",
    "\n",
    "<img src=\"asset/mean_squared_error_img.png\",width=200,height=200, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute mean squared error\n",
    "def compute_error_for_line_given_points(b, w, points):\n",
    "    totalError = 0;\n",
    "    n = len(points)\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        y_prime = (w*x+b)\n",
    "        totalError += (y-y_prime)**2\n",
    "    return totalError / float(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent\n",
    "there are 3 variant of gradient descent algorithm: **Batch Gradient Descent**,  **Stochastic Gradient Descent**,  **Mini-batch Gradient Descent**.\n",
    "\n",
    "for details you can refer to http://sebastianruder.com/optimizing-gradient-descent/index.html#batchgradientdescent\n",
    "\n",
    ">In this notebook, we will implement 2 variants: **Batch Gradient Descent** and **Stochastic Gradient Descent**.\n",
    "\n",
    "Using [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to calculate [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative)\n",
    "\n",
    "<img src=\"asset/gradient_descent_linear_equation_2.png\",width=350,height=350, style=\"float: left;\">\n",
    "<img src=\"asset/gradient_descent_linear_equation_1.png\",width=600,height=600, style=\"float: left;\">\n",
    "<img src=\"asset/gradient_descent_demo.png\",width=800,height=800, style=\"float: left;\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Gradient Descent VS Stochastic Gradient Descent** ?\n",
    "Batch gradient descent computes the gradient using the whole dataset. \n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient using a single sample.\n",
    "\n",
    "In practice, nobody uses Batch Gradient Descent. It's simply too computationally expensive for not that much of a gain. consider minibatch SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in code, batch gradient descent is like below:\n",
    "\n",
    "<img src=\"asset/batch_gradient_descent.png\",width=550,height=550, style=\"float: left;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "def step_batch_gradient(b_current, w_current, points, learningRate):\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    n = len(points)\n",
    "    \n",
    "    # compute gradient of all points with partial derivative\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        w_gradient += (2/n)*(-x)*(y-(w_current*x+b_current))\n",
    "        b_gradient += (2/n)*(-1)*(y-(w_current*x+b_current))\n",
    "    \n",
    "    w_new = w_current - learningRate*w_gradient\n",
    "    b_new = b_current - learningRate*b_gradient\n",
    "    \n",
    "    return [w_new, b_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in code, stochastic gradient descent is like below:\n",
    "\n",
    "<img src=\"asset/stochastic_gradient descent.png\",width=550,height=550, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def step_stochatic_gradient(b_current, w_current, points, learningRate):\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    w_new = w_current\n",
    "    b_new = b_current\n",
    "    n = len(points)\n",
    "    \n",
    "    # compute gradient applied on each point with partial derivative\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        w_gradient += (2/n)*(-x)*(y-(w_current*x+b_current))\n",
    "        b_gradient += (2/n)*(-1)*(y-(w_current*x+b_current))\n",
    "        w_new = w_new - learningRate*w_gradient\n",
    "        b_new = w_new - learningRate*b_gradient\n",
    "    \n",
    "    return [w_new, b_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training process \n",
    "it is to minize the error function and get the best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training process to minize error function with batch gradient descent\n",
    "def batch_gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations, loss_values):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_batch_gradient(b, w, np.array(points), learning_rate)\n",
    "        loss_values.append(compute_error_for_line_given_points(b, w, points))\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training process to minize error function with stochastic gradient descent\n",
    "def stochastic_gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations, loss_values):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_stochatic_gradient(b, w, np.array(points), learning_rate)\n",
    "        loss_values.append(compute_error_for_line_given_points(b, w, points))\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 50 iterations b = 1.4510195572404034, m = 1.4510680113390042, error = 111.87217649923548\n",
      "\n",
      "Starting stochastic gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 50 iterations b = 1.4227968974861496, m = 1.422800206131609, error = 113.94151915627774\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPEREpNMUHiIi0mn/A9lzdVXNeHqEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aaccb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run():\n",
    "    points = np.genfromtxt(\"data/data_linear_regression.csv\", delimiter=\",\")\n",
    "    \n",
    "    # using batch gradient descent\n",
    "    loss_values = []\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_w = 0 # initial slope guess\n",
    "    num_iterations = 50\n",
    "    print(\"Starting batch gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, w] = batch_gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations, loss_values)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))\n",
    "    plt.plot(loss_values, label=\"batch\")\n",
    "    \n",
    "    print(\"\")\n",
    "    # using stochastic gradient descent\n",
    "    loss_values = []\n",
    "    learning_rate = 0.000001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_w = 0 # initial slope guess\n",
    "    num_iterations = 50\n",
    "    print(\"Starting stochastic gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, w] = stochastic_gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations, loss_values)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))\n",
    "    plt.plot(loss_values, label=\"stochastic\")\n",
    "    \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"error\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Classification of Gradient Descent with simplest NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make dummy variables for rank, Convert categorical variable into dummy/indicator variables\n",
    "admissions = pd.read_csv('data/data_sigmoid_classification.csv')\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"asset/sigmoid_gradient.png\",width=550,height=550, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function using mean squ\n",
    "def batch_gradient_descent(weights, features, targets, learnrate):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    # Loop through all records, x is the input, y is the target\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Calculate the output\n",
    "        output = sigmoid(np.dot(x,weights))\n",
    "        # Calculate change in weights. we can also call sigmoid_prime instead of \"output * (1-output)\"\n",
    "        # Simultaneously calculate all features in one data \n",
    "        del_w += (2/n_records) * (y-output) * output * (1-output) * x\n",
    "\n",
    "        # Update weights\n",
    "    weights += learnrate * del_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.245979456349\n",
      "Train loss:  0.241119268558\n",
      "Train loss:  0.237008000385\n",
      "Train loss:  0.233489933426\n",
      "Train loss:  0.23044256818\n",
      "Train loss:  0.227772089607\n",
      "Train loss:  0.22540744723\n",
      "Train loss:  0.223294879997\n",
      "Train loss:  0.221393459701\n",
      "Train loss:  0.219671680965\n"
     ]
    }
   ],
   "source": [
    "n_records, n_features = features.shape\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.01\n",
    "\n",
    "def train():\n",
    "    for e in range(epochs):\n",
    "        batch_gradient_descent(weights, features, targets, learnrate)\n",
    "        # Printing out the mean square error on the training set\n",
    "        if e % (epochs / 10) == 0:\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean((out - targets) ** 2)           \n",
    "            print(\"Train loss: \", loss)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
