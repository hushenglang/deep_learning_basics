{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "This notebook will show you the underlying implementation of Gradient Descent for Linear function and Sigmoid function.\n",
    "\n",
    "If you have any suggestions, welcome to contact [hushenglang@gmail.com](), or pull request on github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear equation\n",
    "> y = wx + b \n",
    "\n",
    "> w is slope, b is y-intercept\n",
    "\n",
    "### error function - mean squared error\n",
    "should be aware that there are lots of error function for different tasks, **mean squared error** function is the one for linear regression\n",
    "<img src=\"asset/mean_squared_error_img.png\",width=200,height=200, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute mean squared error\n",
    "def compute_error_for_line_given_points(b, w, points):\n",
    "    totalError = 0;\n",
    "    n = len(points)\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        y_prime = (w*x+b)\n",
    "        totalError += (y-y_prime)**2\n",
    "    return totalError / float(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent\n",
    "there are 3 variant of gradient descent algorithm: **Batch Gradient Descent**,  **Stochastic Gradient Descent**,  **Mini-batch Gradient Descent**.\n",
    "\n",
    "for details you can refer to http://sebastianruder.com/optimizing-gradient-descent/index.html#batchgradientdescent\n",
    "\n",
    ">In this notebook, we will implement 2 variants: **Batch Gradient Descent** and **Stochastic Gradient Descent**.\n",
    "\n",
    "Using [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to calculate [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative)\n",
    "\n",
    "<img src=\"asset/gradient_descent_linear_equation_2.png\",width=350,height=350, style=\"float: left;\">\n",
    "<img src=\"asset/gradient_descent_linear_equation_1.png\",width=600,height=600, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Gradient Descent VS Stochastic Gradient Descent** ?\n",
    "Batch gradient descent computes the gradient using the whole dataset. \n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient using a single sample.\n",
    "\n",
    "In practice, nobody uses Batch Gradient Descent. It's simply too computationally expensive for not that much of a gain. consider minibatch SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in code, batch gradient descent is like below:\n",
    "\n",
    "<img src=\"asset/batch_gradient_descent.png\",width=550,height=550, style=\"float: left;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "def step_batch_gradient(b_current, w_current, points, learningRate):\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    n = len(points)\n",
    "    \n",
    "    # compute gradient of all points with partial derivative\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        w_gradient += (2/n)*(-x)*(y-(w_current*x+b_current))\n",
    "        b_gradient += (2/n)*(-1)*(y-(w_current*x+b_current))\n",
    "    \n",
    "    w_new = w_current - learningRate*w_gradient\n",
    "    b_new = b_current - learningRate*b_gradient\n",
    "    \n",
    "    return [w_new, b_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in code, stochastic gradient descent is like below:\n",
    "\n",
    "<img src=\"asset/stochastic_gradient descent.png\",width=550,height=550, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def step_stochatic_gradient(b_current, w_current, points, learningRate):\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    w_new = w_current\n",
    "    b_new = b_current\n",
    "    n = len(points)\n",
    "    \n",
    "    # compute gradient applied on each point with partial derivative\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        w_gradient += (2/n)*(-x)*(y-(w_current*x+b_current))\n",
    "        b_gradient += (2/n)*(-1)*(y-(w_current*x+b_current))\n",
    "        w_new = w_new - learningRate*w_gradient\n",
    "        b_new = w_new - learningRate*b_gradient\n",
    "    \n",
    "    return [w_new, b_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training process \n",
    "it is to minize the error function and get the best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training process to minize error function with batch gradient descent\n",
    "def batch_gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations, loss_values):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_batch_gradient(b, w, np.array(points), learning_rate)\n",
    "        loss_values.append(compute_error_for_line_given_points(b, w, points))\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training process to minize error function with stochastic gradient descent\n",
    "def stochastic_gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations, loss_values):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_stochatic_gradient(b, w, np.array(points), learning_rate)\n",
    "        loss_values.append(compute_error_for_line_given_points(b, w, points))\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 50 iterations b = 1.4510195572404034, m = 1.4510680113390042, error = 111.87217649923548\n",
      "\n",
      "Starting stochastic gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 50 iterations b = 1.4227968974861496, m = 1.422800206131609, error = 113.94151915627774\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne3oLSdk0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3290b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run():\n",
    "    points = np.genfromtxt(\"data/data_linear_regression.csv\", delimiter=\",\")\n",
    "    \n",
    "    # using batch gradient descent\n",
    "    loss_values = []\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_w = 0 # initial slope guess\n",
    "    num_iterations = 50\n",
    "    print(\"Starting batch gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, w] = batch_gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations, loss_values)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))\n",
    "    plt.plot(loss_values, label=\"batch\")\n",
    "    \n",
    "    print(\"\")\n",
    "    # using stochastic gradient descent\n",
    "    loss_values = []\n",
    "    learning_rate = 0.000001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_w = 0 # initial slope guess\n",
    "    num_iterations = 50\n",
    "    print(\"Starting stochastic gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, w] = stochastic_gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations, loss_values)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))\n",
    "    plt.plot(loss_values, label=\"stochastic\")\n",
    "    \n",
    "    plt.legend(loc=\"upper right\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Classification of Gradient Descent with simplest NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
